{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "significant-softball",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "clinical-drama",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "stainless-combining",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space Discrete(4)\n",
      "State space Box(-inf, inf, (8,), float32)\n"
     ]
    }
   ],
   "source": [
    "#Actions: 0 - do nothing\n",
    "#         1 - Fire right engine\n",
    "#         2 - Fire main engine\n",
    "#         3 - Fire left engine\n",
    "print(\"Action space {}\".format(env.action_space))\n",
    "print(\"State space {}\".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "according-triple",
   "metadata": {},
   "source": [
    "#Perform random actions (for comparison)\n",
    "env.reset()\n",
    "for i in range(500):\n",
    "    env.render()\n",
    "    observation, reward, done, info = env.step(env.action_space.sample())\n",
    "    \n",
    "    if done:\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "excited-stevens",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(mat):\n",
    "    return np.multiply(mat, (mat>0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "italian-professional",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_derivative(mat):\n",
    "    return (mat>0)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "greatest-equivalent",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNLayer:\n",
    "    #Represents a neural net layer\n",
    "    def __init__(self, input_size, output_size, activation=None, lr=0.001):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.weights = np.random.uniform(low=-0.5, high=0.5, size=(input_size, output_size))\n",
    "        self.activation_function = activation\n",
    "        self.lr = lr\n",
    "    \n",
    "    def update_weights(self, gradient):\n",
    "        self.weights = self.weights - self.lr*gradient\n",
    "    \n",
    "    #Backpropagate this layer\n",
    "    def backward(self, gradient_from_above):\n",
    "        adjusted_mul = gradient_from_above\n",
    "        \n",
    "        #this is pointwise\n",
    "        if self.activation_function != None:\n",
    "            adjusted_mul = np.multiply(relu_derivative(self.backward_store_out),gradient_from_above)\n",
    "            \n",
    "        #Derivative of the loss function with respect to weights\n",
    "        D_i = np.dot(np.transpose(np.reshape(self.backward_store_in, (1, len(self.backward_store_in)))), np.reshape(adjusted_mul, (1,len(adjusted_mul))))\n",
    "\n",
    "        #Calculated error\n",
    "        delta_i = np.dot(adjusted_mul, np.transpose(self.weights))[:-1]\n",
    "\n",
    "        self.update_weights(D_i)\n",
    "        return delta_i\n",
    "    \n",
    "    # Compute the forward pass for this layer\n",
    "    def forward(self, inputs, remember_for_backprop=True):\n",
    "        #Append a bias term to the input\n",
    "        input_with_bias = np.append(inputs, 1)\n",
    "        \n",
    "        #Product of the input and the weight matrix at this layer\n",
    "        unactivated = np.dot(input_with_bias, self.weights)\n",
    "        \n",
    "        #Send output through an activation function (if defined)\n",
    "        output = unactivated\n",
    "        if self.activation_function != None:\n",
    "            output = self.activation_function(output)\n",
    "        \n",
    "        if remember_for_backprop:\n",
    "            #store variables for backward pass\n",
    "            self.backward_store_in = input_with_bias\n",
    "            self.backward_store_out = np.copy(unactivated)\n",
    "    \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "played-enterprise",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEMP\n",
    "class RLAgent:\n",
    "    # class representing a reinforcement learning agent\n",
    "    env = None\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.hidden_size = 24\n",
    "        self.input_size = env.observation_space.shape[0]\n",
    "        self.output_size = env.action_space.n\n",
    "        self.num_hidden_layers = 2\n",
    "        self.epsilon = 1.0\n",
    "        self.memory = deque([],1000000)\n",
    "        self.gamma = 0.95\n",
    "        \n",
    "        self.layers = [NNLayer(self.input_size + 1, self.hidden_size, activation=relu)]\n",
    "        for i in range(self.num_hidden_layers-1):\n",
    "            self.layers.append(NNLayer(self.hidden_size+1, self.hidden_size, activation=relu))\n",
    "        self.layers.append(NNLayer(self.hidden_size+1, self.output_size))\n",
    "        \n",
    "    def select_action(self, observation):\n",
    "        values = self.forward(np.asmatrix(observation))\n",
    "        if (np.random.random() > self.epsilon):\n",
    "            return np.argmax(values)\n",
    "        else:\n",
    "            return np.random.randint(self.env.action_space.n)\n",
    "            \n",
    "    def forward(self, observation, remember_for_backprop=True):\n",
    "        vals = np.copy(observation)\n",
    "        index = 0\n",
    "        for layer in self.layers:\n",
    "            vals = layer.forward(vals, remember_for_backprop)\n",
    "            index = index + 1\n",
    "        return vals\n",
    "        \n",
    "    def remember(self, done, action, observation, prev_obs):\n",
    "        self.memory.append([done, action, observation, prev_obs])\n",
    "        \n",
    "    def experience_replay(self, update_size=20):\n",
    "        if (len(self.memory) < update_size):\n",
    "            return\n",
    "        else: \n",
    "            batch_indices = np.random.choice(len(self.memory), update_size)\n",
    "            for index in batch_indices:\n",
    "                done, action_selected, new_obs, prev_obs = self.memory[index]\n",
    "                action_values = self.forward(prev_obs, remember_for_backprop=True)\n",
    "                next_action_values = self.forward(new_obs, remember_for_backprop=False)\n",
    "                experimental_values = np.copy(action_values)\n",
    "                if done:\n",
    "                    experimental_values[action_selected] = -1\n",
    "                else:\n",
    "                    experimental_values[action_selected] = 1 + self.gamma*np.max(next_action_values)\n",
    "                self.backward(action_values, experimental_values)\n",
    "        self.epsilon = self.epsilon if self.epsilon < 0.01 else self.epsilon*0.997\n",
    "        for layer in self.layers:\n",
    "            layer.lr = layer.lr if layer.lr < 0.0001 else layer.lr*0.99\n",
    "        \n",
    "    def backward(self, calculated_values, experimental_values): \n",
    "        # values are batched = batch_size x output_size\n",
    "        delta = (calculated_values - experimental_values)\n",
    "        # print('delta = {}'.format(delta))\n",
    "        for layer in reversed(self.layers):\n",
    "            delta = layer.backward(delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "interstate-hungary",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLAgent:\n",
    "    env = None\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.hidden_size = 24\n",
    "        self.input_size = env.observation_space.shape[0]\n",
    "        self.output_size = env.action_space.n\n",
    "        self.num_hidden_layers = 2\n",
    "        self.epsilon = 1.0\n",
    "        \n",
    "        self.layers = [NNLayer(self.input_size + 1, self.hidden_size, activation=relu)]\n",
    "        for i in range(self.num_hidden_layers-1):\n",
    "            self.layers.append(NNLayer(self.hidden_size+1, self.hidden_size, activation=relu))\n",
    "        self.layers.append(NNLayer(self.hidden_size+1, self.output_size))\n",
    "        \n",
    "        self.memory = deque([],1000000)\n",
    "        self.gamma = 0.95\n",
    "    \n",
    "    def remember(self, done, action, observation, prev_obs):\n",
    "        self.memory.append([done, action, observation, prev_obs])\n",
    "    \n",
    "    def experience_replay(self, update_size=20):\n",
    "        if len(self.memory) < update_size:\n",
    "            return\n",
    "        \n",
    "        #Randomly sample from all stored memories\n",
    "        batch_indices = np.random.choice(len(self.memory), update_size)\n",
    "        for index in batch_indices:\n",
    "            done, action_selected, new_obs, prev_obs = self.memory[index]\n",
    "            \n",
    "            action_values = self.forward(prev_obs, remember_for_backprop=True)\n",
    "            next_action_values = self.forward(new_obs, remember_for_backprop=False)\n",
    "            experimental_values = np.copy(action_values)\n",
    "            if done:\n",
    "                experimental_values[action_selected] = -1\n",
    "            else:\n",
    "                experimental_values[action_selected] = 1 + self.gamma*np.max(next_action_values)\n",
    "            \n",
    "            #Backpropagate\n",
    "            self.backward(action_values, experimental_values)\n",
    "            \n",
    "        self.epsilon = self.epsilon if self.epsilon < 0.01 else self.epsilon*0.996\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            layer.lr = layer.lr if layer.lr < 0.001 else layer.lr*0.995\n",
    "    \n",
    "    #Backpropagate\n",
    "    def backward(self, calculated_values, experimental_values):\n",
    "        delta = (calculated_values - experimental_values)\n",
    "        for layer in reversed(self.layers):\n",
    "            delta = layer.backward(delta)\n",
    "    \n",
    "    #Feeds forward the input observations through the network to get values for each action\n",
    "    def forward(self, observation, remember_for_backprop=True):\n",
    "        vals = np.copy(observation)\n",
    "        index = 0\n",
    "        for layer in self.layers:\n",
    "            vals = layer.forward(vals, remember_for_backprop)\n",
    "            index += 1\n",
    "        return vals\n",
    "    \n",
    "    #Select an action based on the observation\n",
    "    def select_action(self, observation):\n",
    "        values = self.forward(np.asmatrix(observation))\n",
    "        \n",
    "        if np.random.random() > self.epsilon:\n",
    "            #Best action\n",
    "            return np.argmax(values)\n",
    "        else:\n",
    "            #Random action\n",
    "            return np.random.randint(self.env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "announced-valuation",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 ended after 157 timesteps\n",
      "Episode 1 ended after 100 timesteps\n",
      "Episode 2 ended after 78 timesteps\n",
      "Episode 3 ended after 60 timesteps\n",
      "Episode 4 ended after 71 timesteps\n",
      "Episode 5 ended after 116 timesteps\n",
      "Episode 6 ended after 76 timesteps\n",
      "Episode 7 ended after 72 timesteps\n",
      "Episode 8 ended after 133 timesteps\n",
      "Episode 9 ended after 148 timesteps\n",
      "Episode 10 ended after 147 timesteps\n",
      "Episode 11 ended after 173 timesteps\n",
      "Episode 12 ended after 142 timesteps\n",
      "Episode 13 ended after 82 timesteps\n",
      "Episode 14 ended after 103 timesteps\n",
      "Episode 15 ended after 128 timesteps\n",
      "Episode 16 ended after 107 timesteps\n",
      "Episode 17 ended after 112 timesteps\n",
      "Episode 18 ended after 85 timesteps\n",
      "Episode 19 ended after 101 timesteps\n",
      "Episode 20 ended after 108 timesteps\n",
      "Episode 21 ended after 111 timesteps\n",
      "Episode 22 ended after 219 timesteps\n",
      "Episode 23 ended after 86 timesteps\n",
      "Episode 24 ended after 137 timesteps\n",
      "Episode 25 ended after 235 timesteps\n",
      "Episode 26 ended after 84 timesteps\n",
      "Episode 27 ended after 132 timesteps\n",
      "Episode 28 ended after 103 timesteps\n",
      "Episode 29 ended after 213 timesteps\n",
      "Episode 30 ended after 141 timesteps\n",
      "Episode 31 ended after 151 timesteps\n",
      "Episode 32 ended after 108 timesteps\n",
      "Episode 33 ended after 169 timesteps\n",
      "Episode 34 ended after 174 timesteps\n",
      "Episode 35 ended after 116 timesteps\n",
      "Episode 36 ended after 203 timesteps\n",
      "Episode 37 ended after 228 timesteps\n",
      "Episode 38 ended after 103 timesteps\n",
      "Episode 39 ended after 171 timesteps\n",
      "Episode 40 ended after 174 timesteps\n",
      "Episode 41 ended after 117 timesteps\n",
      "Episode 42 ended after 145 timesteps\n",
      "Episode 43 ended after 132 timesteps\n",
      "Episode 44 ended after 129 timesteps\n",
      "Episode 45 ended after 178 timesteps\n",
      "Episode 46 ended after 117 timesteps\n",
      "Episode 47 ended after 142 timesteps\n",
      "Episode 48 ended after 113 timesteps\n",
      "Episode 49 ended after 300 timesteps\n",
      "Episode 50 ended after 236 timesteps\n",
      "Episode 51 ended after 134 timesteps\n",
      "Episode 52 ended after 199 timesteps\n",
      "Episode 53 ended after 205 timesteps\n",
      "Episode 54 ended after 179 timesteps\n",
      "Episode 55 ended after 140 timesteps\n",
      "Episode 56 ended after 133 timesteps\n",
      "Episode 57 ended after 124 timesteps\n",
      "Episode 58 ended after 129 timesteps\n",
      "Episode 59 ended after 195 timesteps\n",
      "Episode 60 ended after 118 timesteps\n",
      "Episode 61 ended after 199 timesteps\n",
      "Episode 62 ended after 78 timesteps\n",
      "Episode 63 ended after 133 timesteps\n",
      "Episode 64 ended after 124 timesteps\n",
      "Episode 65 ended after 119 timesteps\n",
      "Episode 66 ended after 115 timesteps\n",
      "Episode 67 ended after 100 timesteps\n",
      "Episode 68 ended after 94 timesteps\n",
      "Episode 69 ended after 151 timesteps\n",
      "Episode 70 ended after 91 timesteps\n",
      "Episode 71 ended after 70 timesteps\n",
      "Episode 72 ended after 239 timesteps\n",
      "Episode 73 ended after 193 timesteps\n",
      "Episode 74 ended after 78 timesteps\n",
      "Episode 75 ended after 112 timesteps\n",
      "Episode 76 ended after 110 timesteps\n",
      "Episode 77 ended after 125 timesteps\n",
      "Episode 78 ended after 136 timesteps\n",
      "Episode 79 ended after 105 timesteps\n",
      "Episode 80 ended after 114 timesteps\n",
      "Episode 81 ended after 158 timesteps\n",
      "Episode 82 ended after 169 timesteps\n",
      "Episode 83 ended after 253 timesteps\n",
      "Episode 84 ended after 139 timesteps\n",
      "Episode 85 ended after 159 timesteps\n",
      "Episode 86 ended after 121 timesteps\n",
      "Episode 87 ended after 135 timesteps\n",
      "Episode 88 ended after 110 timesteps\n",
      "Episode 89 ended after 208 timesteps\n",
      "Episode 90 ended after 126 timesteps\n",
      "Episode 91 ended after 103 timesteps\n",
      "Episode 92 ended after 114 timesteps\n",
      "Episode 93 ended after 110 timesteps\n",
      "Episode 94 ended after 93 timesteps\n",
      "Episode 95 ended after 121 timesteps\n",
      "Episode 96 ended after 94 timesteps\n",
      "Episode 97 ended after 84 timesteps\n",
      "Episode 98 ended after 78 timesteps\n",
      "Episode 99 ended after 130 timesteps\n",
      "Episode 100 ended after 126 timesteps\n",
      "Episode 101 ended after 131 timesteps\n",
      "Episode 102 ended after 133 timesteps\n",
      "Episode 103 ended after 167 timesteps\n",
      "Episode 104 ended after 147 timesteps\n",
      "Episode 105 ended after 202 timesteps\n",
      "Episode 106 ended after 159 timesteps\n",
      "Episode 107 ended after 185 timesteps\n",
      "Episode 108 ended after 123 timesteps\n",
      "Episode 109 ended after 192 timesteps\n",
      "Episode 110 ended after 215 timesteps\n",
      "Episode 111 ended after 202 timesteps\n",
      "Episode 112 ended after 305 timesteps\n",
      "Episode 113 ended after 169 timesteps\n",
      "Episode 114 ended after 144 timesteps\n",
      "Episode 115 ended after 128 timesteps\n",
      "Episode 116 ended after 242 timesteps\n",
      "Episode 117 ended after 202 timesteps\n",
      "Episode 118 ended after 94 timesteps\n",
      "Episode 119 ended after 170 timesteps\n",
      "Episode 120 ended after 93 timesteps\n",
      "Episode 121 ended after 230 timesteps\n",
      "Episode 122 ended after 144 timesteps\n",
      "Episode 123 ended after 400 timesteps\n",
      "Episode 124 ended after 146 timesteps\n",
      "Episode 125 ended after 181 timesteps\n",
      "Episode 126 ended after 171 timesteps\n",
      "Episode 127 ended after 126 timesteps\n",
      "Episode 128 ended after 177 timesteps\n",
      "Episode 129 ended after 157 timesteps\n",
      "Episode 130 ended after 107 timesteps\n",
      "Episode 131 ended after 136 timesteps\n",
      "Episode 132 ended after 363 timesteps\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-f37142de45be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m#Iterating through time steps within an episode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMAX_TIMESTEPS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mprev_obs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobservation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dennis\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode, **kwargs)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'human'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 240\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dennis\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\gym\\envs\\box2d\\lunar_lander.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    374\u001b[0m                                      color=(0.8, 0.8, 0))\n\u001b[0;32m    375\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 376\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'rgb_array'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    377\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dennis\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, return_rgb_array)\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0monetime_geoms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mreturn_rgb_array\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misopen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dennis\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pyglet\\window\\win32\\__init__.py\u001b[0m in \u001b[0;36mflip\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    334\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_always_dwm\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dwm_composition_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_interval\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 336\u001b[1;33m                     \u001b[0m_dwmapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDwmFlush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "NUM_EPISODES = 10000\n",
    "MAX_TIMESTEPS = 5000\n",
    "model = RLAgent(env)\n",
    "\n",
    "#Main loop\n",
    "for i_episode in range(NUM_EPISODES):\n",
    "    observation = env.reset()\n",
    "    \n",
    "    #Iterating through time steps within an episode\n",
    "    for t in range(MAX_TIMESTEPS):\n",
    "        env.render()\n",
    "        action = model.select_action(observation)\n",
    "        prev_obs = observation\n",
    "        observation, reward, done, info = env.step(action)\n",
    "\n",
    "        #Keep a store of the agent's experiences\n",
    "        model.remember(done, action, observation, prev_obs)\n",
    "        model.experience_replay(64)\n",
    "        \n",
    "        #epsilon decay\n",
    "        model.epsilon = model.epsilon if model.epsilon < 0.01 else model.epsilon*0.995\n",
    "        \n",
    "        if done:\n",
    "            print(\"Episode {} ended after {} timesteps\".format(i_episode, t+1))\n",
    "            #print(\"Episode {} ended after {} timesteps, current exploration is {}\".format(i_episode, t+1, model.epsilon))\n",
    "            break\n",
    "            \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "prompt-carroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Play\n",
    "env.reset()\n",
    "\n",
    "for t in range(1000):\n",
    "    env.render()\n",
    "    action = model.select_action(observation)\n",
    "    observation, reward, done, info = env.step(action)\n",
    "\n",
    "    if done:\n",
    "        break\n",
    "            \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "further-landing",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
